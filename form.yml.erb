# Batch Connect app configuration file
#
# @note Used to define the submitted cluster, title, description, and

cluster: "quests"

form:
  # What Comsol version do you want to use
  - version
  # determines the size of your desktop window
  - vnc_resolution
  # This allows the user to select which partition they submit to
  - slurm_partition
  # This allows a user to request a GPU
  - gres_value
  # This allows a user to specify the account they are submitting under
  - slurm_account
  # This allows a user to use more than 1 core
  - num_cores
  # This checkbox is to encourage folks to consider before asking for more than 1 node
  - request_more_than_one_node
  # how many nodes do you want to request
  - number_of_nodes
  # This allows a user to request RAM
  - memory_per_node
  # How many hours do you want to run this job for
  - bc_num_hours
  # User can supply e-mail if they would like to be e-mailed when the session begins
  - user_email
  # Allow the user to provide a Slurm "constraint" option
  - constraint
  # Name of the Slurm job
  - job_name
  # these variables are for the JavaScript
  - raw_data
  - raw_group_data
  - bc_vnc_resolution

attributes:
  version:
    widget: select
    label: "ANSYS or ANSYSEDT Module"
    help: "This defines the version of COMSOL you want to load."
    options:
      - [ "ANSYS Workbench 2025 R1", "ansys/2025R1" ]
      - [ "ANSYS Workbench 2023 R2", "ansys/2023R2" ]
      - [ "ANSYS Electronics Desktop 2023R2", "ansysedt/2023R2" ]

  vnc_resolution:
    label: "Resolution"
    required: true
    widget: "select"
    value: "1280x1024"
    options:
      - ["2560x1600","2560x1600"]
      - ["2560x1440","2560x1440"]
      - ["1920x1200","1920x1200"]
      - ["1920x1080","1920x1080"]
      - ["1600x1200","1600x1200"]
      - ["1280x1024","1280x1024"]
      - ["1280x960","1280x960"]
      - ["1024x768","1024x768"]
      - ["800x600","800x600"]
    help: |
      The resolution setting (in pixels width x height) for the VNC connection.

  bc_vnc_resolution:
    required: true
    value: "1280x1024"
    help: |
      The resolution setting (in pixels width x height) for the VNC connection.

  bc_num_hours:
    label: "Wall Time (in number of hours)"
    help: |
      Select the maximum number of *hours* you want this session/job to last. You can always terminate your session/job early, but it will automatically be terminated by Slurm once the walltime is hit. This will set the value of the `--time` flag for your job. Your available options for walltime are effected by the partition that you have selected. For more information on walltimes, please see the [walltime section](https://services.northwestern.edu/TDClient/30/Portal/KB/ArticleDet?ID=1964#section-walltimes) of our Everything You Need to Know about Using Slurm on Quest page.
    value: 1

  slurm_partition: 
    label: "SLURM Partition"
    help: |
      Select the SLURM partition you will submit this job to. This will set the value of the `-p` or `--partition` flag for your job. This selection will impact a number of other options on this form, including which accounts/allocations and maximum walltimes you can choose and whether or not you have the option to select GPUs. For more information on partitions, please see the [partitions section](https://services.northwestern.edu/TDClient/30/Portal/KB/ArticleDet?ID=1964#section-partitions) of our Everything You Need to Know about Using Slurm on Quest page.
    widget: select

  slurm_account:
    label: "SLURM Account"
    help: |
      The Quest allocation/account under which you will be submitting this job. This will set the value of the `-A` or `--account` flag for your job. *Please note that if you do not see an allocation option you expect, this means one of the following:* that it is a storage only allocation, that your allocation is expired, in which case please request to renew it using the appropriate form that can be [found here](https://www.it.northwestern.edu/secure/forms/research/allocation-request-forms.html), or that the partition that you have selected cannot be submitted to from that allocation. More information on this parameter can be found in the [account section](https://services.northwestern.edu/TDClient/30/Portal/KB/ArticleDet?ID=1964#section-account) of our Everything You Need to Know about Using Slurm on Quest page.
    widget: select

  gres_value:
    label: "GPU(s) that you would like to request"
    help: |
      The number and type of GPUs you are requesting for this job, only available for certain partitions. This will set the value of the `--gres` flag for your job. More information on GPUs can be found on the [GPUs on Quest page](https://services.northwestern.edu/TDClient/30/Portal/KB/ArticleDet?ID=1112).
    widget: select
    default: ""

  num_cores:
    widget: "number_field"
    label: "Number of CPUs/cores/processors"
    value: 1
    help: |
      The number of CPUs you want for this session. This will set the value of the `--ntasks-per-node` flag for your job. Please note that you likely do not need to request more than a single CPU unless you know that your application can be parallelized. Also, please note that the more cores requested, the longer the wait will be for the session to start. For more information on setting the number of cores, please see the [number of cores section](https://services.northwestern.edu/TDClient/30/Portal/KB/ArticleDet?ID=1964#section-number-of-cores) of our Everything You Need to Know about Using Slurm on Quest page.
    min: 1
    max: 64
    step: 1
    id: 'num_cores'

  request_more_than_one_node:
    widget: "check_box"
    label: "Request more than a single node (Optional)"
    help: |
      Only check this box if you are confident that your application of choice can utilize CPUs across more than one node. In Python, some of these applications include MPI4py, Dask, and PySpark. Once checked, you will be shown another field where you can request multiple nodes.

  number_of_nodes:
    widget: "number_field"
    label: "Number of nodes being requested."
    value: 1
    min: 1
    step: 1
    id: 'number_of_nodes'
    help: |
      How many nodes you want for this session. This will set the value of the `-N/--nodes` flag for your job. A reminder to check that your application of choice can utilize CPUs across more than one node. These applications/packages would include MPI4py, Dask, PySpark, etc. For more information on setting the number of nodes, please see the [number of nodes section](https://services.northwestern.edu/TDClient/30/Portal/KB/ArticleDet?ID=1964#section-number-of-nodes) of our Everything You Need to Know about Using Slurm on Quest page.

  memory_per_node:
    widget: "number_field"
    label: "Total memory or RAM do you need in GB."
    value: 5
    help: |
      How much total memory/RAM (in GBs) you want for this session. This will set the value of the `--mem` flag for your job. Knowing ahead of time how much memory you need can be difficult. That said, you can use the utility `seff` to profile *completed* jobs on Slurm which give the maximum memory used by that job. For more information on setting memory, please see the [memory section](https://services.northwestern.edu/TDClient/30/Portal/KB/ArticleDet?ID=1964#section-required-memory) of our Everything You Need to Know about Using Slurm on Quest page.
    min: 1
    max: 243
    step: 1
    id: 'memory_per_node'

  user_email:
    label: "Email Address (Optional)"
    help: |
      Enter your email address if you would like to receive an email when the session starts. Leave blank for no email.

  constraint:
    label: "Constraint (Optional)"
    help: |
      Not all Quest compute nodes are the same. We currently have four different generations or architectures of compute nodes which we refer to as quest9, quest10, quest11, and quest12. This will set the value of the `--constraint` flag for your job. For more information on setting the constraint option, please see the [constraint section](https://services.northwestern.edu/TDClient/30/Portal/KB/ArticleDet?ID=1964#section-constraints) of our Everything You Need to Know about Using Slurm on Quest page.
    widget: "select"

  job_name:
    label: "Name of the Job"
    value: "ANSYS Workbench or Electronics Desktop GUI"

  raw_data:
    label: "Account associations for use by JavaScript"
    widget: hidden_field
    value: "<%= RawSlurmData.read_sinfo_from_file %>"
    cacheable: false

  raw_group_data:
    label: "linux groups for use by JavaScript"
    widget: hidden_field
    value: "<%= RawSlurmData.rawgroupdata %>"
    cacheable: false
